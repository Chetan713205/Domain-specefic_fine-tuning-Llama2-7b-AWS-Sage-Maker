# ğŸ‡®ğŸ‡³ **Indian Constitutional Law â€” Domainâ€‘Specific Finetuning (Llamaâ€‘2â€‘7Bâ€‘Chat)**

> ğŸ§ âš–ï¸ Highâ€‘quality legal assistant fineâ€‘tuned on Indian Constitutional Law to deliver precise, citationâ€‘aware, and safetyâ€‘conscious answers.

[![Model](https://img.shields.io/badge/HF%20Model-Available-ffcc00?logo=huggingface)](https://huggingface.co/chetantiwari/Llama-2-7b-chat-finetune)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](#license)
[![Transformers](https://img.shields.io/badge/Transformers-%F0%9F%A6%84-0?logo=python\&labelColor=black\&color=green)](https://github.com/huggingface/transformers)

---

## âœ¨ Overview

This repository hosts a fineâ€‘tuned **Llamaâ€‘2â€‘7Bâ€‘Chat** model specialized in **Indian Constitutional Law**. The model has been adapted on a curated corpus of constitutional provisions, case law summaries, doctrine explanations, and notable amendments to assist with:

* ğŸ“š **Doctrinal explanations** (e.g., Basic Structure, Separation of Powers, Fundamental Rights)
* ğŸ” **Provision lookup & synthesis** (Articles, Parts, Schedules, Amendments)
* ğŸ›ï¸ **Caseâ€‘aware reasoning** (e.g., *Kesavananda Bharati*, *Maneka Gandhi*, *SR Bommai*)
* ğŸ§© **Plainâ€‘language summaries** for students & practitioners
* ğŸ›¡ï¸ **Safetyâ€‘aligned outputs** with disclaimers & refusal policies for legal advice

> ğŸš€ **Deployed model**: **Hugging Face** â†’ **[`chetantiwari/Llama-2-7b-chat-finetune`](https://huggingface.co/chetantiwari/Llama-2-7b-chat-finetune)**

---

## ğŸ—ï¸ Project Goals

* Create a **domainâ€‘expert assistant** for Indian Constitutional Law.
* Encourage **transparent, citationâ€‘ready outputs**.
* Provide a **practical baseline** for further legalâ€‘domain finetunes (e.g., Criminal/Contract/Administrative Law).

---

## ğŸ—‚ï¸ Dataset (Highâ€‘Level)

> *Note: Only public, educational, or permissibleâ€‘use texts were included. Proprietary or copyrighted sources were **excluded**.*

* ğŸ”– **Statutory content**: Constitution of India (Articles, Schedules, Amendments) â€” normalized & sectionâ€‘indexed.
* ğŸ“œ **Landmark case briefs**: concise holdings, issues, ratios, and principles.
* ğŸ§­ **Doctrines & tests**: Reasonable Classification, Proportionality, Wednesbury, Basic Structure, etc.
* ğŸ§¹ **Preâ€‘processing**: deâ€‘duplication, regex cleanup, section tagging, articleâ†”topic mapping.
* ğŸª„ **Instruction format**: (system, user, assistant) chat triples + chainâ€‘ofâ€‘thought **not** included in supervised targets.

> If you want to extend/replace the dataset, see **`data/`** format spec below.

---

## ğŸ”§ Training Pipeline

* **Base model**: `meta-llama/Llama-2-7b-chat`
* **Method**: Parameterâ€‘efficient finetuning (e.g., **LoRA/QLoRA**) for compute efficiency
* **Precision**: 4â€‘bit or 8â€‘bit weights for memoryâ€‘constrained training
* **Loss**: Supervised crossâ€‘entropy on instructionâ€‘tuned chat format
* **Eval**: Heldâ€‘out prompts covering Articles, rights limitations, and doctrine applications
* **Safety**: Refusal templates for legal advice & enforcement of disclaimers

**Example training config (pseudo)**

```yaml
model_name: meta-llama/Llama-2-7b-chat
method: qlora
bnb_4bit: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
max_seq_len: 4096
per_device_train_batch_size: 8
gradient_accumulation_steps: 4
lr: 2e-4
epochs: 3
warmup_ratio: 0.03
save_strategy: steps
logging_steps: 25
```

---

## ğŸ§ª Evaluation (Illustrative)

* âœ… **Article retrieval fidelity** (does the response cite correct Articles/Parts?)
* âœ… **Doctrinal accuracy** (correct test/application summaries)
* âœ… **Case consistency** (names, years, holdings not hallucinated)
* âš ï¸ **Safety**: Refuses to provide formal legal advice; adds disclaimers

**Sample rubric**

* 0â€“2: Incorrect / hallucinated
* 3â€“4: Partially correct with gaps
* 5: Accurate, sourced, concise

---

## ğŸš€ Quick Start

### 1) Install

```bash
pip install transformers accelerate bitsandbytes einops sentencepiece
```

### 2) Load from Hugging Face

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

MODEL_ID = "chetantiwari/Llama-2-7b-chat-finetune"

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16,
    device_map="auto"
)

def chat(prompt, system="You are a helpful assistant on Indian Constitutional Law."):
    # Simple chat template
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.2, top_p=0.9)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(chat("Explain the Basic Structure doctrine."))
```

### 3) Hugging Face Inference API (cURL)

```bash
curl -X POST \
  -H "Authorization: Bearer $HF_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"inputs": "Explain Article 21 and its expansion via Maneka Gandhi."}' \
  https://api-inference.huggingface.co/models/chetantiwari/Llama-2-7b-chat-finetune
```

> ğŸ’¡ Tip: Keep **temperature low (0.1â€“0.3)** for factâ€‘heavy answers.

---

## ğŸ—£ï¸ Prompting Guide

* âœ… Ask **focused questions**: *â€œWhat is the scope of Article 19(1)(a) free speech, and what are the reasonable restrictions?â€*
* âœ… Request **short citations**: *â€œCite relevant Articles and landmark cases.â€*
* âœ… Prefer **stepwise reasoning** for complex issues: *â€œList tests â†’ apply to facts â†’ conclude.â€*
* âŒ Avoid asking for **legal advice** or strategies for ongoing litigation.

**Example prompts**

```text
Summarize the evolution of Article 21 post-Maneka Gandhi in 5 bullet points.
```

```text
Compare basic structure review vs. ordinary judicial review with 3 case examples.
```

---

## ğŸ“¦ Repository Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ constitution_articles.jsonl      # {id, article_no, text, topics}
â”‚   â”œâ”€â”€ cases_summaries.jsonl            # {case_name, year, issue, holding}
â”‚   â””â”€â”€ doctrines.jsonl                  # {doctrine, definition, key_cases}
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ config.yaml                      # sample training config
â”‚   â”œâ”€â”€ prepare_dataset.py               # cleaning & chat-formatting
â”‚   â”œâ”€â”€ train_qlora.py                   # PEFT training loop
â”‚   â””â”€â”€ eval.py                          # lightweight eval metrics
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ main.ipynb                       # end-to-end demo (preprocess â†’ train â†’ eval)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ merge_lora_weights.py            # optional: merge + push to HF
â”œâ”€â”€ app/
â”‚   â””â”€â”€ demo.py                          # (optional) Gradio/Streamlit app
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ›¡ï¸ Safety, Use & Limitations

* âš ï¸ **Not legal advice.** Outputs are for **education & research** only.
* ğŸ§ª May hallucinate case names or dates â€” **verify before use**.
* ğŸŒ Jurisdictionâ€‘specific nuances: model is **Indiaâ€‘centric**; not tuned for other jurisdictions.

**Recommended system prompt**

```text
You are a domain-specific assistant for Indian Constitutional Law. Provide concise, neutral answers with relevant Articles, Parts, Schedules, and 1â€“3 landmark cases. Add a disclaimer: â€œEducational use only, not legal advice.â€
```

---

## ğŸ§° Reproducibility Notes

* Set `TORCH_DISABLE_MPS_FALLBACK=1` (macOS) if needed.
* Use `--gradient_checkpointing` for memory savings on long contexts.
* Determinism: `torch.manual_seed(42)` + controlled sampling parameters.

---

## ğŸ—ºï¸ Roadmap

* [ ] Expand dataset: recent Supreme Court constitutional benches
* [ ] RAG connector with SCC/Manupatraâ€‘style citations (public alternatives only)
* [ ] Multiâ€‘task finetune with QA + summarization + classification heads
* [ ] Robust eval set with **articleâ€‘span grounding**
* [ ] Distilled 3B/1.3B variants for onâ€‘device use

---

## ğŸ¤ Contributing

Contributions are welcome! Please open an **Issue** or **Pull Request** with:

* Clear description of changes
* Source of any added legal content (must be public/permitted)
* Minimal examples or unit tests where applicable

---

## ğŸ“œ License

This project is released under the **Apache 2.0 License**. See `LICENSE` for more details. Dataset components must respect their original licenses.

---

## ğŸ™ Acknowledgements

* Meta for Llamaâ€‘2 base model
* Hugging Face ecosystem (Transformers, Datasets, PEFT)
* Indian legal scholarship & public resources used for education

---

## ğŸ”– Citation

```bibtex
@software{tiwari2025indianconstllm,
  author  = {Chetan Tiwari},
  title   = {Indian Constitutional Law â€” Domain-Specific Finetuning (Llama-2-7B-Chat)},
  year    = {2025},
  url     = {https://huggingface.co/chetantiwari/Llama-2-7b-chat-finetune}
}
```

---

## ğŸŒŸ Star & Share

If this helped you, please â­ the repo and share with fellow learners & practitioners! ğŸ™Œ
